{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Negative Sampling\n",
    "\n",
    "reference: https://arxiv.org/pdf/1310.4546.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Index and Frequency Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_map_from_file(file_name, threshold):\n",
    "    \"\"\"\n",
    "    Build dictionary and count frequency for vocabulary in file.\n",
    "\n",
    "    Arguments:\n",
    "    file_name -- name of the training sample file.\n",
    "    threshold -- lower bound of frequency of words to be included\n",
    "\n",
    "    Returns:\n",
    "    word2idx_map -- dictionary of {word : index}\n",
    "    idx2freq_map -- list\n",
    "    idx2word_map - list\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        # count the frequency of words\n",
    "        count = Counter()\n",
    "        for line in f.readlines():\n",
    "            for w in line.decode('utf-8').strip().split():\n",
    "                count.update(w)\n",
    "                \n",
    "    # sort the words by its frequency in ascending order\n",
    "    word_count_list = list(filter(lambda x: x[1] >= threshold, count.most_common()))\n",
    "    \n",
    "    # dict {word : index}\n",
    "    word2idx_map = {w: idx for idx, (w, _) in enumerate(word_count_list)}\n",
    "    \n",
    "    # list\n",
    "    idx2freq_map = list(map(lambda x : x[1], word_count_list))\n",
    "    \n",
    "    # list\n",
    "    idx2word_map = list(map(lambda x : x[0], word_count_list))\n",
    "    return word2idx_map, idx2freq_map, idx2word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx_map, idx2freq_map, idx2word_map = get_word_map_from_file('poem.txt', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram model\n",
    "\n",
    "$$\\mathcal{L} = \\sum_t \\sum_{-c \\le j \\le c, j \\ne 0} \\mathcal{L}(w_{t+j}, w_j)$$\n",
    "\n",
    "## Objective of Skip-gram with Negative Sampling\n",
    "\n",
    "$$P(w_{context} \\mid w_{target}) = \\sigma(u_{w_{context}}^T v_{w_{target}}) $$\n",
    "\n",
    "$$P_{neg}(w_i) = \\frac{F(w_i)^{\\frac{3}{4}}}{\\sum_i F(w_i)^{\\frac{3}{4}}}\\tag{1}$$\n",
    "\n",
    "$$ \\mathcal{L}(w_{out}, w_{in}) = -log(P(w_{out} \\mid w_{in})) - \\sum_{j=1}^{k} \\mathbb {E}_{w_j \\backsim  P_{neg}(w_j)} log(1-P(w_j\\mid w_{in})) $$\n",
    "\n",
    "## Subsampling\n",
    "\n",
    "$$ P(w_i) = (\\sqrt{\\frac {F(w_i)}{0.001}} + 1)⋅\\frac {0.001}{F(w_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sampling_table(idx2freq_map, power=0.75, sample=1e-3):\n",
    "    \"\"\"\n",
    "    Build unigram table for vocab.\n",
    "\n",
    "    Arguments:\n",
    "    power -- the power to resample vocab frequency\n",
    "\n",
    "    Returns:\n",
    "    neg_table -- list of index with resampled frequency\n",
    "    keep_prob -- numpy array of shape (n_words, )\n",
    "\n",
    "    \"\"\"\n",
    "    neg_freq_list = list(map(lambda x: (x[0], int(x[1] ** power)),\n",
    "                             enumerate(idx2freq_map)))\n",
    "    table_size = sum([x[1] for x in neg_freq_list])\n",
    "    neg_table = np.zeros(table_size).astype(int)\n",
    "    offset = 0\n",
    "    for word, freq in neg_freq_list:\n",
    "        neg_table[offset : offset + freq] = word\n",
    "        offset += freq\n",
    "\n",
    "    assert(offset == table_size)\n",
    "    \n",
    "    z = np.array(idx2freq_map) / (sample * sum(idx2freq_map))\n",
    "    keep_prob = (np.sqrt(z) + 1) / z\n",
    "    \n",
    "    return neg_table, keep_prob\n",
    "\n",
    "def init_parameters(vocab_size, embed_size, use_biases=True, init='zero'):\n",
    "    '''\n",
    "    Initialize parameters for NN model.\n",
    "\n",
    "    Argumemts:\n",
    "    vocab_size -- count of distinct input vocab.\n",
    "    embed_size -- the vector length to embed word to.\n",
    "    use_biases -- whether to use the bias term in model.\n",
    "    init --\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dict of model parameters.\n",
    "                  parameters['embeddings'] -- (embed_size, vocab_size) matrix.\n",
    "                  parameters['weights'] -- (vocab_size, embed_size) matrix.\n",
    "                  (optional)\n",
    "                  parameters['biases'] -- (vocab_size, 1) matrix.\n",
    "    '''\n",
    "    parameters = {}\n",
    "    parameters['embeddings'] = np.random.randn(embed_size, vocab_size) / np.sqrt(vocab_size)\n",
    "    if init == 'zero':\n",
    "        parameters['weights'] = np.zeros((vocab_size, embed_size))\n",
    "    elif init == 'xavier':\n",
    "        parameters['weights'] = np.random.randn(vocab_size, embed_size) / np.sqrt(embed_size)\n",
    "    if use_biases:\n",
    "        parameters['biases'] = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def sigmoid(logits):\n",
    "    return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "def forward_prop(parameters):\n",
    "    '''\n",
    "    Get probabilistic predictions of model.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dict of model parameters to train.\n",
    "\n",
    "    Returns:\n",
    "    pred -- numpy array of shape (n_samples, 1).\n",
    "\n",
    "    '''\n",
    "    preds = sigmoid(np.dot(parameters['weights'], parameters['embeddings']) +\n",
    "                    (parameters['biases'] if 'baises' in parameters else 0))\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def back_prop(preds, labels, parameters):\n",
    "    '''\n",
    "    Get model's gradients.\n",
    "\n",
    "    Arguments:\n",
    "    preds -- (n_samples, 1) vector of probabilistic predictions.\n",
    "    labels -- (n_samples, 1) vector of labels.\n",
    "    parameters -- dictionary of model parameters to train.\n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary of gradients\n",
    "             grads['embeddings'] -- (embed_size, 1) vector.\n",
    "             grads['weights'] -- (n_samples, embed_size) vector.\n",
    "             grads['biases'] -- (n_samples, 1) vector.\n",
    "    \n",
    "    '''\n",
    "    grads = {}\n",
    "    ds = preds - labels # (n_samples, 1) vector\n",
    "    grads['embeddings'] = np.dot(parameters['weights'].T, ds)\n",
    "    grads['weights'] = np.dot(ds, parameters['embeddings'].T)\n",
    "    if 'biases' in parameters:\n",
    "        grads['biases'] = ds\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "$$g_t = \\nabla_\\theta \\mathcal{L}(\\theta_{t-1})$$\n",
    "\n",
    "- ### Gradient Descent\n",
    "$$\\theta_t = \\theta_{t - 1} - \\alpha g_t$$\n",
    "\n",
    "- ### Adam\n",
    "reference: https://arxiv.org/pdf/1412.6980v8.pdf\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "$$\\widehat{m}_t = \\frac{m_t}{1 - \\beta_1^{t}}$$\n",
    "$$\\widehat{v}_t = \\frac{v_t}{1 - \\beta_2^{t}}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\widehat{m}_t}{\\sqrt{\\widehat{v}_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_proj(parameters, word_pairs):\n",
    "    '''\n",
    "    find the corresponding projection of word vectors of input word pairs.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dict of model parameters.\n",
    "    word_pairs -- tuple (input_word, target_words)\n",
    "                  input_word -- (1, 1) vector of input word index.\n",
    "                  target_words -- (n_samples, 1) vector of output word index.\n",
    "                  (due to negative sampling.)\n",
    "\n",
    "    Returns:\n",
    "    parameters_proj -- dict of projected parameters\n",
    "                       parameters_proj['embeddings'] -- numpy array of shape (embed_size, 1).\n",
    "                       parameters_proj['weights'] -- numpy array of shape (n_samples, embed_size).\n",
    "                       (optional) parameters_proj['biases'] -- numpy array of shape (n_samples, 1).\n",
    "    '''\n",
    "    (input_word, target_words) = word_pairs\n",
    "    parameters_proj = {}\n",
    "    parameters_proj['embeddings'] = parameters['embeddings'][:, input_word]\n",
    "    parameters_proj['weights'] = parameters['weights'][target_words, :]\n",
    "    if 'biases' in parameters:\n",
    "        parameters_proj['biases'] = parameters['biases'][target_words, :]\n",
    "        \n",
    "    return parameters_proj\n",
    "        \n",
    "\n",
    "def set_proj(parameters, parameters_proj, word_pairs):\n",
    "    '''\n",
    "    set the projection back to original parameters dict.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dict of model parameters.\n",
    "    parameters_proj -- dict of projected model parameters.\n",
    "\n",
    "    '''\n",
    "    (input_word, target_words) = word_pairs\n",
    "    parameters['embeddings'][:, input_word] = parameters_proj['embeddings']\n",
    "    parameters['weights'][target_words, :] = parameters_proj['weights']\n",
    "    if 'biases' in parameters:\n",
    "        parameters['biases'][target_words, :] = parameters_proj['biases']\n",
    "\n",
    "class GradientDescentOptimzer:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def update_proj(self, parameters, grads_proj, word_pairs):\n",
    "        '''\n",
    "        Arguments:\n",
    "        parameters -- dictionary\n",
    "        grads_proj -- dictionary\n",
    "        word_pairs -- tuple\n",
    "        '''\n",
    "        parameters_proj = get_proj(parameters, word_pairs)\n",
    "        \n",
    "        for key in parameters_proj.keys():\n",
    "            parameters_proj[key] -= self.alpha * grads_proj[key]\n",
    "            \n",
    "        set_proj(parameters, parameters_proj, word_pairs)\n",
    "\n",
    "        \n",
    "class Adam:\n",
    "    def __init__(self, alpha, parameters, beta1=0.9, beta2=0.999, epsilon=1e-8, decay=0.0):\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.t = 1\n",
    "        for key, param in parameters.items():\n",
    "            self.m[key] = np.zeros_like(param)\n",
    "            self.v[key] = np.zeros_like(param)\n",
    "        \n",
    "    def update_proj(self, parameters, grads_proj, word_pairs):\n",
    "        '''\n",
    "        Arguments:\n",
    "        parameters -- dictionary\n",
    "        grads_proj -- dictionary\n",
    "        word_pairs -- tuple\n",
    "        '''\n",
    "        m_proj = get_proj(self.m, word_pairs)\n",
    "        v_proj = get_proj(self.v, word_pairs)\n",
    "        parameters_proj = get_proj(parameters, word_pairs)\n",
    "        \n",
    "        lr = self.alpha * (np.sqrt(1. - np.power(self.beta2, self.t)) /\n",
    "             (1. - np.power(self.beta1, self.t)))\n",
    "        if self.decay > 0:\n",
    "            lr /= 1 + self.decay * (self.t - 1)\n",
    "        \n",
    "        for key in parameters.keys():\n",
    "            m_proj[key] = self.beta1 * m_proj[key] + (1. - self.beta1) * grads_proj[key]\n",
    "            v_proj[key] = self.beta2 * v_proj[key] + (1. - self.beta2) * np.square(grads_proj[key])\n",
    "            parameters_proj[key] -= lr * m_proj[key] / (np.sqrt(v_proj[key]) + self.epsilon)\n",
    "        \n",
    "        set_proj(self.m, m_proj, word_pairs)\n",
    "        set_proj(self.v, v_proj, word_pairs)\n",
    "        set_proj(parameters, parameters_proj, word_pairs)\n",
    "        self.t += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2index(line, word2idx_map, keep_prob):\n",
    "    sen = []\n",
    "    for w_raw in line.decode(\"utf-8\").strip().split():\n",
    "        # get index from dict\n",
    "        w_idx = word2idx_map.get(w_raw, -1)\n",
    "        # word not in dic (frequency < threshold)\n",
    "        if w_idx == -1:\n",
    "            continue\n",
    "        # randomly subsamples word due to its freq\n",
    "        if keep_prob[w_idx] < random.random():\n",
    "            continue\n",
    "        sen.append(w_idx)\n",
    "        \n",
    "    return sen\n",
    "\n",
    "\n",
    "def negative_sampling(context, sen_set, k, neg_table):\n",
    "    target_words = set()\n",
    "\n",
    "    # positive example\n",
    "    # pred(input_word, context) should be 1\n",
    "    \n",
    "    while len(target_words) < k:\n",
    "        # out of sentence negative example from table\n",
    "        # pred(input_word, neg) should be 0\n",
    "        while True:\n",
    "            neg_word = np.random.choice(neg_table)\n",
    "            if neg_word not in sen_set:\n",
    "                break\n",
    "        target_words.add(neg_word)\n",
    "    \n",
    "    #labels = np.zeros((k + 1, 1))\n",
    "    #labels[0] = 1.0\n",
    "    \n",
    "    return [context] + list(target_words), np.eye(k + 1)[:, [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_embedding_to_file(model_name, data, epoch):\n",
    "    '''\n",
    "    Arguments:\n",
    "    model_name -- string\n",
    "    data -- numpy array\n",
    "    epoch -- int\n",
    "    '''\n",
    "    path = 'output/' + model_name\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    file_name = path + \"/epoch%s.json\" % epoch\n",
    "\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(json.dumps(data.tolist(), indent=4))\n",
    "        \n",
    "def load_embedding_from_file(file_name):\n",
    "    '''\n",
    "\n",
    "    Arguments:\n",
    "    file_name -- the json file name saving results.\n",
    "\n",
    "    Returns:\n",
    "    '''\n",
    "    with open(file_name, \"r\") as f:\n",
    "        embeddings = np.array(json.loads(\"\".join(f.readlines())))\n",
    "    return embeddings\n",
    "\n",
    "def train_embedding(model_name, file_name, word_map=None,\n",
    "    embed_size=30, window=2,\n",
    "    use_biases=False,\n",
    "    optimizer='gd',\n",
    "    learning_rate=0.025,\n",
    "    sample=1e-3,\n",
    "    negative_samples=15,\n",
    "    n_epochs=2, \n",
    "    verbose=1,\n",
    "    init='zero'):\n",
    "\n",
    "    start_time = time.time()\n",
    "    if not word_map:\n",
    "        print('Build word map from file:', file_name)\n",
    "        word2idx_map, idx2freq_map, _ = get_word_map_from_file(\"poem.txt\")\n",
    "    else:\n",
    "        print('Load prebuilt word map')\n",
    "        word2idx_map, idx2freq_map = word_map\n",
    "        \n",
    "    \n",
    "    total_words = sum(idx2freq_map)\n",
    "    vocab_size = len(idx2freq_map)\n",
    "    print(\"Model %s, optimizer: %s start\" % (model_name, optimizer))\n",
    "    print(\"Total Words: %d, Vocabulary: %d, Embedded to: %d-d vectors\\n\"\n",
    "          % (total_words, vocab_size, embed_size))\n",
    "\n",
    "    neg_table, keep_prob = create_sampling_table(idx2freq_map, sample=sample)\n",
    "    parameters = init_parameters(vocab_size, embed_size, use_biases, init)\n",
    "    \n",
    "    if optimizer == 'gd':\n",
    "        optimizer = GradientDescentOptimzer(learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate, parameters)\n",
    "    else:\n",
    "        print('Unknown Optimzer!!')\n",
    "        raise ValueError\n",
    "        \n",
    "    train_words = 0\n",
    "\n",
    "    steps = 0\n",
    "    avg_loss = 0.\n",
    "    err_count = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(\"Epoch%d start\" % epoch)\n",
    "        with open(file_name, 'rb') as f:\n",
    "        # train through sentence\n",
    "            for line in f:\n",
    "                # list of word's idx in sentence\n",
    "                sen = sentence2index(line, word2idx_map, keep_prob)\n",
    "                sen_set = set(sen)\n",
    "                train_words += len(sen)\n",
    "\n",
    "                for center, context in enumerate(sen):\n",
    "                    # for words in window\n",
    "                    for input_word in range(max(0, center - window),\n",
    "                                            min(len(sen), center + window + 1)):\n",
    "                        if input_word == center:\n",
    "                            continue\n",
    "\n",
    "                        target_words, labels = negative_sampling(context, sen_set, negative_samples, neg_table)\n",
    "                        word_pairs = ([sen[input_word]], target_words)\n",
    "\n",
    "                        parameters_proj = get_proj(parameters, word_pairs)\n",
    "\n",
    "                        preds = forward_prop(parameters_proj)\n",
    "                        loss = -np.log(abs(preds + labels - 1))\n",
    "\n",
    "                        # gradient descent\n",
    "                        grads_proj = back_prop(preds, labels, parameters_proj)\n",
    "                        optimizer.update_proj(parameters, grads_proj, word_pairs)\n",
    "\n",
    "                        avg_loss += np.sum(loss)\n",
    "                        err_count += len(target_words)\n",
    "                        steps += 1\n",
    "                        if steps % 10000 == 0 and verbose >= 2:\n",
    "                            print(\"Epoch%d, Trained Words: %d, Average Loss: %.4f, Time Cost: %.2f s\" %\n",
    "                                  (epoch, train_words, avg_loss / err_count, time.time() - start_time))\n",
    "                            avg_loss = 0.\n",
    "                            err_count = 0\n",
    "\n",
    "            if verbose >= 1:\n",
    "                print(\"Epoch%d finished, Trained Words: %d, Average Loss: %.4f, Time Cost: %.2f s\" %\n",
    "                      (epoch, train_words, avg_loss / err_count, time.time() - start_time))\n",
    "                avg_loss = 0.\n",
    "                err_count = 0\n",
    "\n",
    "        # output to .json file\n",
    "        save_embedding_to_file(model_name, parameters['embeddings'], epoch)\n",
    "            \n",
    "        print(\"Build file: \", model_name)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load prebuilt word map\n",
      "Model try, optimizer: gd start\n",
      "Total Words: 2593895, Vocabulary: 5500, Embedded to: 30-d vectors\n",
      "\n",
      "Epoch1 start\n",
      "Epoch1, Trained Words: 3625, Average Loss: 0.6931, Time Cost: 1.32 s\n",
      "Epoch1, Trained Words: 7194, Average Loss: 0.6926, Time Cost: 2.51 s\n",
      "Epoch1, Trained Words: 10828, Average Loss: 0.6750, Time Cost: 3.69 s\n",
      "Epoch1, Trained Words: 14437, Average Loss: 0.5634, Time Cost: 4.98 s\n",
      "Epoch1, Trained Words: 17921, Average Loss: 0.4328, Time Cost: 6.23 s\n",
      "Epoch1, Trained Words: 21810, Average Loss: 0.3178, Time Cost: 7.50 s\n",
      "Epoch1, Trained Words: 25633, Average Loss: 0.2749, Time Cost: 8.74 s\n"
     ]
    }
   ],
   "source": [
    "train_embedding('try', 'poem.txt', (word2idx_map, idx2freq_map), learning_rate=0.025, optimizer='gd', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = load_embedding_from_file('output/test/epoch2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top(word, n_top=5):\n",
    "\n",
    "    norm = np.linalg.norm(embeddings, axis=0)\n",
    "    if isinstance(word, str):\n",
    "        idx = word2idx_map.get(word)\n",
    "        vec = embeddings[:, idx]\n",
    "        norm_w = norm[idx]\n",
    "    else:\n",
    "        idx = -1\n",
    "        vec = word\n",
    "        norm_w = np.linalg.norm(vec)\n",
    "\n",
    "    product = np.dot(embeddings.T, vec)\n",
    "    cosine = product / (norm * norm_w)\n",
    "\n",
    "    ret = heapq.nlargest(n_top,\n",
    "                         filter(lambda x : x[0] != idx,\n",
    "                                [(x[0], x[1]) for x in enumerate(cosine)]),\n",
    "                         key=lambda x : x[1])\n",
    "\n",
    "    if isinstance(word, str):\n",
    "        print(word)\n",
    "    for x in ret:\n",
    "        print(idx2word_map[x[0]], \"%.3f\" % x[1])\n",
    "\n",
    "def get_vec(word, axis=1):\n",
    "    return embeddings[:, word2idx_map.get(word)]\n",
    "\n",
    "def get_similarity(w1, w2):\n",
    "    if isinstance(w1, str):\n",
    "        w1 = get_vec(w1)\n",
    "    if isinstance(w2, str):\n",
    "        w2 = get_vec(w2)\n",
    "    norm1 = np.linalg.norm(w1)\n",
    "    norm2 = np.linalg.norm(w2)\n",
    "    return np.dot(w1, w2)/ (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 0.710\n",
      "干 0.689\n",
      "怒 0.674\n",
      "快 0.660\n",
      "甯 0.659\n",
      "籲 0.646\n",
      "激 0.636\n",
      "淳 0.635\n",
      "禍 0.628\n",
      "既 0.617\n"
     ]
    }
   ],
   "source": [
    "get_top(get_vec(u\"大\") - get_vec(u\"小\") + get_vec(u\"慢\"), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "弓\n",
      "雕 0.845\n",
      "鋒 0.826\n",
      "刀 0.825\n",
      "彎 0.825\n",
      "珂 0.821\n",
      "將\n",
      "教 0.775\n",
      "須 0.771\n",
      "成 0.736\n",
      "休 0.734\n",
      "攻 0.732\n",
      "歸\n",
      "來 0.864\n",
      "去 0.799\n",
      "還 0.760\n",
      "緱 0.746\n",
      "游 0.745\n",
      "微\n",
      "霏 0.789\n",
      "熏 0.697\n",
      "暄 0.678\n",
      "颸 0.674\n",
      "濃 0.673\n",
      "鬥\n",
      "射 0.835\n",
      "鬣 0.799\n",
      "嚼 0.796\n",
      "勒 0.790\n",
      "鐙 0.781\n"
     ]
    }
   ],
   "source": [
    "get_top(u\"弓\")\n",
    "get_top(u\"將\")\n",
    "get_top(u\"歸\")\n",
    "get_top(u\"微\")\n",
    "get_top(u\"鬥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data_to_file(file_name, data):\n",
    "    path = 'output/'\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    with open(path + file_name, \"w\") as f_out:\n",
    "        f_out.write(json.dumps(data, indent=4))\n",
    "        \n",
    "def load_data_from_file(file_name):\n",
    "    with open('output/' + file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_data_to_file('word2idx_map', word2idx_map)\n",
    "save_data_to_file('idx2word_map', idx2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx_map = load_data_from_file('word2idx_map')\n",
    "idx2word_map = load_data_from_file('idx2word_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
